{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd \nfrom skimage import io\nfrom skimage import color\nfrom PIL import Image\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dask.array.image import imread\nfrom dask import bag, threaded\nfrom dask.diagnostics import ProgressBar\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Flatten, Dense,GlobalAveragePooling2D\nfrom keras.layers import Flatten,Dropout\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image \nfrom keras.layers.normalization import BatchNormalization\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# import pandas as pd\n# driver_imgs_list = pd.read_csv(\"../input/state-farm-distracted-driver-detection/driver_imgs_list.csv\")\n# sample_submission = pd.read_csv(\"../input/state-farm-distracted-driver-detection/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"driver_details =pd.read_csv(\"../input/state-farm-distracted-driver-detection/driver_imgs_list.csv\")\nprint(driver_details.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting all the images\n\ntrain_image = []\nimage_label = []\n\n\nfor i in range(10):\n    print('now we are in the folder C',i)\n    imgs = os.listdir(\"../input/state-farm-distracted-driver-detection/imgs/train/c\"+str(i))\n    for j in range(1100):#len(imgs)):\n    #for j in range(100):\n        img_name = \"../input/state-farm-distracted-driver-detection/imgs/train/c\"+str(i)+\"/\"+imgs[j]\n        img = cv2.imread(img_name)\n        #img = color.rgb2gray(img)\n        img = img[50:,120:-50]\n        img = cv2.resize(img,(224,224))\n        label = i\n        driver = driver_details[driver_details['img'] == imgs[j]]['subject'].values[0]\n        train_image.append([img,label,driver])\n        image_label.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Randomly shuffling the images\n\nimport random\nrandom.shuffle(train_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"driv_selected = ['p050', 'p015', 'p022', 'p056']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Splitting the train and test\n\nX_train= []\ny_train = []\nX_test = []\ny_test = []\nD_train = []\nD_test = []\n\nfor features,labels,drivers in train_image:\n    if drivers in driv_selected:\n        X_test.append(features)\n        y_test.append(labels)\n        D_test.append(drivers)\n    \n    else:\n        X_train.append(features)\n        y_train.append(labels)\n        D_train.append(drivers)\n    \nprint (len(X_train),len(X_test))\nprint (len(y_train),len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Converting images to nparray. Encoding the Y\n\nX_train = np.array(X_train).reshape(-1,224,224,3)\nX_test = np.array(X_test).reshape(-1,224,224,3)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n\nprint (X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Defining the input\n\nfrom keras.layers import Input\nvgg16_input = Input(shape = (224, 224, 3), name = 'Image_input')\n\n\n## The VGG model\n\nfrom keras.applications.vgg16 import VGG16, preprocess_input\n\n#Get back the convolutional part of a VGG network trained on ImageNet\nmodel_vgg16_conv = VGG16(weights='imagenet', include_top=False, input_tensor = vgg16_input)\nmodel_vgg16_conv.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the generated model \nfrom keras.models import Model\n\n\noutput_vgg16_conv = model_vgg16_conv(vgg16_input)\n\n#Add the fully-connected layers \nx=GlobalAveragePooling2D()(output_vgg16_conv)\nx=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\nx = Dropout(0.1)(x) # **reduce dropout \nx=Dense(1024,activation='relu')(x) #dense layer 2\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nx = Dense(512,activation='relu')(x) #dense layer 3\nx = Dense(10, activation='softmax', name='predictions')(x)\n\nvgg16_pretrained = Model(input = vgg16_input, output = x)\nvgg16_pretrained.summary()\n\n# Compile CNN model\nsgd = optimizers.SGD(lr = 0.001)\nvgg16_pretrained.compile(loss='categorical_crossentropy',optimizer = sgd,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\n\ncheckpointer = ModelCheckpoint('vgg_weights_aug_setval_layers_sgd2.hdf5', verbose=1, save_best_only=True)\nearlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n\n\ndatagen = ImageDataGenerator(\n    height_shift_range=0.5,\n    width_shift_range = 0.5,\n    zoom_range = 0.5,\n    rotation_range=30\n        )\n#datagen.fit(X_train)\ndata_generator = datagen.flow(X_train, y_train, batch_size = 64)\n\n# Fits the model on batches with real-time data augmentation:\nvgg16_model = vgg16_pretrained.fit_generator(data_generator,steps_per_epoch = len(X_train) / 64, callbacks=[checkpointer, earlystopper],\n                                                            epochs = 50, verbose = 1, validation_data = (X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (10, 5))\naxes[0].plot(range(1, len(vgg16_pretrained.history.history['accuracy']) + 1), vgg16_pretrained.history.history['accuracy'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Accuracy')\naxes[0].plot(range(1, len(vgg16_pretrained.history.history['val_accuracy']) + 1), vgg16_pretrained.history.history['val_accuracy'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Accuracy')\naxes[0].set_xlabel('Epochs', fontsize = 14)\naxes[0].set_ylabel('Accuracy',fontsize = 14)\naxes[0].set_title('CNN Dropout Accuracy Trainig VS Testing', fontsize = 14)\naxes[0].legend(loc = 'best')\naxes[1].plot(range(1, len(vgg16_pretrained.history.history['loss']) + 1), vgg16_pretrained.history.history['loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Loss')\naxes[1].plot(range(1, len(vgg16_pretrained.history.history['val_loss']) + 1), vgg16_pretrained.history.history['val_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Loss')\naxes[1].set_xlabel('Epochs', fontsize = 14)\naxes[1].set_ylabel('Loss',fontsize = 14)\naxes[1].set_title('CNN Dropout Loss Trainig VS Testing', fontsize = 14)\naxes[1].legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels is the image array\ntest_image = []\ni = 0\nfig, ax = plt.subplots(1, 20, figsize = (50,50 ))\n\nfiles = os.listdir('../input/state-farm-distracted-driver-detection/imgs/test')\nnums = np.random.randint(low=1, high=len(files), size=20)\nfor i in range(20):\n    print ('Image number:',i)\n    img = cv2.imread('../input/state-farm-distracted-driver-detection/imgs/test/'+files[nums[i]])\n    #img = color.rgb2gray(img)\n    img = img[50:,120:-50]\n    img = cv2.resize(img,(224,224))\n    test_image.append(img)\n    ax[i].imshow(img,cmap = 'gray')\n    plt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = []\n\nfor img in test_image:\n    test.append(img)\n    \nvgg16_pretrained.load_weights('vgg_weights_aug_setval_layers_sgd2.hdf5')\n\n\ntest = np.array(test).reshape(-1,224,224,3)\nprediction = vgg16_pretrained.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = { \"C0\": \"safe driving\",\n\"C1\": \"texting - right\",\n\"C2\": \"talking on the phone - right\",\n\"C3\": \"texting - left\",\n\"C4\": \"talking on the phone - left\",\n\"C5\": \"operating the radio\",\n\"C6\": \"drinking\",\n\"C7\": \"reaching behind\",\n\"C8\": \"hair and makeup\",\n\"C9\": \"talking to passenger\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels is the image array\ni = 0\nfig, ax = plt.subplots(20, 1, figsize = (100,100))\n\nfor i in range(20):\n    ax[i].imshow(test[i].squeeze())\n    predicted_class = 'C'+str(np.where(prediction[i] == np.amax(prediction[i]))[0][0])\n    ax[i].set_title(tags[predicted_class])\n    plt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}